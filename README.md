전체 시스템 아키텍처
Docker Compose 구성:

NGINX: 리버스 프록시 및 로드밸런서 역할
Legacy App: 기존 안정 버전 (포트 8080, 8081)
Refactored App: 새로운 리팩토링 버전 (포트 9080, 9081)
Prometheus: 메트릭 수집 및 저장
Grafana: 시각화 대시보드
Alertmanager: 알람 및 웹훅 처리

Docker Compose 설정 구조
네트워크 구성:
모든 컨테이너를 동일한 Docker 네트워크에 배치하여 서비스 디스커버리 활용. NGINX가 upstream 설정에서 컨테이너명으로 백엔드 서버 참조 가능.
볼륨 마운트:

NGINX 설정 파일을 호스트와 공유하여 런타임 중 가중치 변경 가능
Prometheus 데이터 영속성을 위한 볼륨
Grafana 대시보드 설정 자동 로드를 위한 프로비저닝 볼륨

NGINX 동적 설정 변경 메커니즘
템플릿 기반 설정 관리:
NGINX 설정을 템플릿화하여 환경변수로 가중치 조정. Controller 서비스가 환경변수 업데이트 후 nginx -s reload 명령으로 무중단 설정 적용.
헬스체크 엔드포인트 활용:
각 애플리케이션에 /health, /ready 엔드포인트 구현. NGINX의 proxy_next_upstream 설정으로 실패한 백엔드 자동 제외.

모니터링 스택 구성
Prometheus 메트릭 수집:

NGINX Exporter로 요청 수, 응답시간, 에러율 수집
각 애플리케이션의 /metrics 엔드포인트에서 비즈니스 메트릭 수집
Docker 컨테이너 리소스 사용량 모니터링

Grafana 대시보드:

실시간 트래픽 분산 현황
버전별 에러율 및 응답시간 비교
시스템 리소스 사용량
카나리 배포 진행 상황 타임라인

점진적 트래픽 증가 전략
단계별 가중치 조정:
초기 5% 트래픽을 리팩토링 버전으로 라우팅 후, 5분 간격으로 메트릭 검증. 에러율 2% 미만, 응답시간 기준값의 150% 미만일 경우 다음 단계로 진행 (10% → 25% → 50% → 100%).
A/B 테스트 기반 검증:
특정 사용자 그룹이나 요청 헤더 기반으로 트래픽 분할. 통계적 유의성 확보를 위한 최소 샘플 수 달성 후 다음 단계 진행.
자동 롤백 시스템
실시간 임계값 모니터링:
Prometheus의 recording rule과 alerting rule을 활용하여 5초 간격으로 메트릭 평가. 연속 3회 임계값 초과 시 알람 발생.
즉시 롤백 프로세스:

Alertmanager 웹훅으로 Controller 서비스에 알람 전송
Controller가 NGINX 설정에서 리팩토링 버전 가중치를 0%로 변경
진행 중인 요청 완료 대기 (최대 30초)
Docker Compose로 리팩토링 컨테이너 graceful shutdown
Grafana에 롤백 이벤트 어노테이션 추가

롤백 후 분석:
롤백 시점의 메트릭 데이터를 자동으로 스냅샷하여 원인 분석을 위한 데이터 보존. 슬랙이나 이메일로 개발팀에 상세 리포트 전송.
Controller 서비스 역할
트래픽 조정 API:
RESTful API로 수동/자동 가중치 조정 기능 제공. /api/traffic/increase, /api/traffic/rollback 엔드포인트로 운영자가 직접 제어 가능.
상태 관리:
현재 배포 단계, 메트릭 상태, 롤백 이력을 Redis나 파일 시스템에 저장하여 재시작 후에도 상태 복구 가능.
안전장치:
동시 배포 방지, 최대 롤백 횟수 제한, 긴급 정지 기능 등의 안전장치 구현.
이러한 구조로 로컬에서 완전한 카나리 배포 시스템을 구축하여 실제 프로덕션 환경과 유사한 테스트가 가능합니다. 특히 Controller 서비스가 모든 로직을 중앙화하여 일관성 있는 배포 프로세스를 보장하는 것이 핵심입니다.


